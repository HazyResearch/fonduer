{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Extracting Images for Transistors from PDF Datasheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "We will walk through the process of using `Fonduer` to extract images from [**richly formatted** data](https://hazyresearch.github.io/snorkel/blog/fonduer.html), where information is conveyed via combinations of textual, structural, tabular, and visual expressions, as seen in webpages, business reports, product specifications, and scientific literature.\n",
    "\n",
    "In this tutorial, we use `Fonduer` to identify mentions of the image of transistors in a corpus of transistor datasheets from [Digikey.com](https://www.digikey.com/products/en/discrete-semiconductor-products/transistors-bipolar-bjt-single/276).\n",
    "\n",
    "The tutorial only contains two parts:\n",
    "\n",
    "1. KBC Initialization\n",
    "2. Candidate Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: KBC Initialization\n",
    "\n",
    "In this first phase of `Fonduer`'s pipeline, `Fonduer` uses a user specified _schema_ to initialize a relational database where the output KB will be stored. Furthermore, `Fonduer` iterates over its input _corpus_ and transforms each document into a unified data model, which captures the variability and multimodality of richly formatted data. This unified data model then servers as an intermediate representation used in the rest of the phases.\n",
    "\n",
    "This preprocessed data is saved to a database. Connection strings can be specified by setting the `SNORKELDB` environment variable. If no database is specified, then SQLite at `./snorkel.db` is created by default. However, to enabled parallel execution, we use PostgreSQL throughout this tutorial.\n",
    "\n",
    "We initialize several variables for convenience that define what the database should be called and what level of parallelization the `Fonduer` pipeline will be run with. In the code below, we use PostgreSQL as our database backend. \n",
    "\n",
    "Before you continue, please make sure that you have PostgreSQL installed and have created a new database named `stg_temp_max_figure`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "PARALLEL = 4 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"stg_temp_max_figure\"\n",
    "\n",
    "os.environ['FONDUERDBNAME'] = ATTRIBUTE\n",
    "os.environ['SNORKELDB'] = 'postgres://localhost:5432/' + os.environ['FONDUERDBNAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Defining a Candidate Schema\n",
    "\n",
    "We first initialize a `SnorkelSession`, which manages the connection to the database automatically, and enables us to save intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the _schema_ of the relation we want to extract. This must be a subclass of Candidate, and we define it using a helper function. Here, we define a binary relation which connects two Span objects of text. This is what creates the relation's database table if it does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "\n",
    "Part_Fig = candidate_subclass('Part_Fig', ['fig'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 1.2 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "Next, we load the corpus of datasheets and transform them into the unified data model. Each datasheet has a PDF and HTML representation. Both representations are used in conjunction to create a robust unified data model with textual, structural, tabular, and visual modality information. Note that since each document is independent of each other, we can parse the documents in parallel. Note that parallel execution will not work with SQLite, the default database engine. We depend on PostgreSQL for this functionality.\n",
    "\n",
    "### Configuring an `HTMLPreprocessor`\n",
    "We start by setting the paths to where our documents are stored, and defining a `HTMLPreprocessor` to read in the documents found in the specified paths. `max_docs` specified the number of documents to parse. For the sake of this tutorial, we only look at 100 documents.\n",
    "\n",
    "**Note that you need to have run `download_data.sh` before executing these next steps or you won't have the documents needed for the tutorial.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "\n",
    "docs_path = os.environ['FONDUERHOME'] + '/tutorials/hardware_image/data/html/'\n",
    "pdf_path = os.environ['FONDUERHOME'] + '/tutorials/hardware_image/data/pdf/'\n",
    "\n",
    "max_docs = 4\n",
    "doc_preprocessor = HTMLPreprocessor(docs_path, max_docs=max_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring an `OmniParser`\n",
    "Next, we configure an `OmniParser`, which serves as our `CorpusParser` for PDF documents. We use [CoreNLP](https://stanfordnlp.github.io/CoreNLP/) as a preprocessing tool to split our documents into phrases and tokens, and to provide annotations such as part-of-speech tags and dependency parse structures for these phrases. In addition, we can specify which modality information to include in the unified data model for each document. Below, we enable all modality information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 3.4 s, sys: 140 ms, total: 3.54 s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = OmniParser(structural=True, lingual=True, visual=True, pdf_path=pdf_path, flatten=[])\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use simple database queries (written in the syntax of [SQLAlchemy](http://www.sqlalchemy.org/), which `Fonduer` uses) to check how many documents and phrases (sentences) were parsed, or even check how many phrases and tables are contained in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Documents:', 4L)\n",
      "('Phrases:', 36527L)\n",
      "('Figures:', 450L)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.models import Document, Phrase, Figure\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Phrases:\", session.query(Phrase).count())\n",
    "print(\"Figures:\", session.query(Figure).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dividing the Corpus into Test and Train\n",
    "\n",
    "We'll split the documents 80/10/10 into train/dev/test splits. Note that here we do this in a non-random order to preverse the consistency in the tutorial, and we reference the splits by 0/1/2 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'lmp2011', u'lmp2014mt']\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.5, 0.75)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "pprint([x.name for x in train_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Candidate Extraction & Multimodal Featurization\n",
    "Given the unified data model from Phase 1, `Fonduer` extracts relation candidates based on user-provided **matchers** and **throttlers**. Then, `Fonduer` leverages the multimodality information captured in the unified data model to provide multimodal features for each candidate.\n",
    "\n",
    "## 2.1 Candidate Extraction\n",
    "\n",
    "The next step is to extract **candidates** from our corpus. A `candidate` is the object for which we want to make predictions. In this case, the candidates are pairs of transistor part numbers and their corresponding maximum storage temperatures as found in their datasheets. Our task is to predict which pairs are true in the associated document.\n",
    "\n",
    "To do so, we write **matchers** to define which spans of text in the corpus are instances of each entity. Matchers can leverage a variety of information from regular expressions, to dictionaries, to user-defined functions. Furthermore, different techniques can be combined to form higher quality matchers. In general, matchers should seek to be as precise as possible while maintaining complete recall.\n",
    "\n",
    "In our case, we need to write a matcher that defines a transistor part number and a matcher to define a valid image in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a image matcher\n",
    "\n",
    "Our image matcher can be a very simple since we want to search all images in the documents. More advanced matchers can be defined by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.fonduer.matchers import *\n",
    "\n",
    "def do_nothing_matcher(fig):\n",
    "    return True\n",
    "\n",
    "fig_matcher = LambdaFunctionFigureMatcher(func=do_nothing_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a relation's `ContextSpaces`\n",
    "\n",
    "Next, in order to define the \"space\" of all candidates that are even considered from the document, we need to define a `ContextSpace` for each component of the relation we wish to extract.\n",
    "\n",
    "In the case of transistor images, the `ContextSpace` can be all png images.\n",
    "\n",
    "When no special preproessing like this is needed, we could have used the default `OmniFigures` class provided by `snorkel.contrib.fonduer.candidates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.candidates import OmniFigures\n",
    "\n",
    "figs = OmniFigures(type='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the `CandidateExtractor`\n",
    "\n",
    "Now, we have all the component necessary to perform candidate extraction. We have defined the \"space\" of things to consider for each candidate, provided matchers that signal when a valid mention is seen, and a throttler to prunes away excess candidates. We now can define the `CandidateExtractor` with the contexts to extract from, the matchers, and the throttler to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 172 ms, sys: 76 ms, total: 248 ms\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.candidates import CandidateExtractor\n",
    "\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Part_Fig, [figs], [fig_matcher])\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specified that these `Candidates` belong to the training set by specifying `split=0`; recall that we're referring to train/dev/test as splits 0/1/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of candidates:', 226)\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(Part_Fig).filter(Part_Fig.split == 0).all()\n",
    "print(\"Number of candidates:\", len(train_cands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating for development and test splits\n",
    "Finally, we rerun the same operation for the other two document divisions: dev and test. For each, we simply load the `Corpus` object and run them through the `CandidateExtractor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "('Number of candidates:', 73L)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "('Number of candidates:', 151L)\n",
      "CPU times: user 1.11 s, sys: 116 ms, total: 1.22 s\n",
      "Wall time: 3.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1)\n",
    "    print(\"Number of candidates:\", session.query(Part_Fig).filter(Part_Fig.split == i+1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part_Fig(Image(Doc: lmp2014mt, Position: 34, Url: lmp2014mt/Image_035.jpg))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "n = 9\n",
    "\n",
    "print(train_cands[n])\n",
    "Image(docs_path + '/' + train_cands[n][0].url, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TBD..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
